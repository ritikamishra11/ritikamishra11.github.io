# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Ensure plots display inline in the notebook
%matplotlib inline

# Upload file (works in Google Colab)
from google.colab import files
uploaded = files.upload()

# Load the uploaded CSV file into a DataFrame
file_path = 'AAPL.csv'  # Change this if your file name differs
data = pd.read_csv(file_path)

# Check the structure of the data to ensure the correct columns are present
print(data.columns)

# Calculate daily returns based on 'Adjusted Price'
if 'Adjusted Price' in data.columns:
    data['Daily Return'] = data['Adjusted Price'].pct_change()
else:
    raise KeyError("The 'Adjusted Price' column is missing from the dataset.")

# Drop rows with missing values in the 'Daily Return' column
data_cleaned = data.dropna(subset=['Daily Return'])

# Calculate the annualized mean return and annualized volatility
mean_daily_return = data_cleaned['Daily Return'].mean()
annualized_return = mean_daily_return * 252  # 252 trading days in a year
std_daily_return = data_cleaned['Daily Return'].std()
annualized_volatility = std_daily_return * np.sqrt(252)

# Print out the annualized return and volatility
print(f"Annualized Mean Return: {annualized_return}")
print(f"Annualized Volatility: {annualized_volatility}")
# Function to calculate and display summary statistics
def summary_statistics(data):
    if 'Adjusted Price' not in data.columns:
        raise KeyError("'Adjusted Price' column is missing in the data.")
    
    # Ensure daily return column exists
    if 'Daily Return' not in data.columns:
        data['Daily Return'] = data['Adjusted Price'].pct_change()

    # Drop NaN values from 'Daily Return'
    data_cleaned = data.dropna(subset=['Daily Return'])
    
    # Calculate summary statistics
    summary_stats = {
        "Mean Price": data['Adjusted Price'].mean(),
        "Median Price": data['Adjusted Price'].median(),
        "Price Standard Deviation": data['Adjusted Price'].std(),
        "Minimum Price": data['Adjusted Price'].min(),
        "Maximum Price": data['Adjusted Price'].max(),
        "Annualized Return": data_cleaned['Daily Return'].mean() * 252,  # 252 trading days
        "Annualized Volatility": data_cleaned['Daily Return'].std() * np.sqrt(252),
        "Skewness": data_cleaned['Daily Return'].skew(),
        "Kurtosis": data_cleaned['Daily Return'].kurtosis(),
        "Max Drawdown": max_drawdown(data['Adjusted Price'])
    }
    
    return summary_stats

# Function to calculate the maximum drawdown (peak-to-trough decline)
def max_drawdown(prices):
    running_max = prices.cummax()  # Calculate the running maximum price
    drawdown = (prices - running_max) / running_max  # Calculate the percentage drawdown
    return drawdown.min()  # Return the largest drawdown

# Call the function and print summary statistics
stats = summary_statistics(data)
for stat, value in stats.items():
    print(f"{stat}: {value}")
# Convert the 'Date' column to datetime for easy filtering by year
data['Date'] = pd.to_datetime(data['Date'])

# Extract the unique years from the 'Date' column
years = data['Date'].dt.year.unique()

# Create a 1x5 grid for visualizing the adjusted price over 5 different years
fig, axes = plt.subplots(1, len(years[:5]), figsize=(14, 4), sharey=True)  # Adjust number of years if needed

# Loop over each unique year and plot the Adjusted Price for that year
for ax, year in zip(axes, years[:5]):  # Limiting to first 5 years for better visualization
    yearly_data = data[data['Date'].dt.year == year]
    
    # Plot Adjusted Price over time for that year
    yearly_data.plot(x='Date', y='Adjusted Price', ax=ax, legend=False)
    
    # Set title and labels
    ax.set_title(f"Year: {year}")
    ax.set_xlabel('Date')
    ax.set_ylabel('Adjusted Price')

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()
