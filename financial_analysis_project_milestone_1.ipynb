{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Financial Analysis Project\n",
        "\n",
        "##Collaboration Plan:\n",
        "Raymond and I plan to meet twice a week to stay on track with our weekly goals. Every Monday, we will discuss what we want to accomplish *that* week and divide the tasks accordingly. We will meet again on Thursday to review progress, address any roadblocks, and plan any additional work required for the weekend. If necessary, we will also meet on Sunday to resolve any remaining issues together."
      ],
      "metadata": {
        "id": "OSofd6I1GKWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Project Description:\n",
        "In this project, we will analyze Apple Inc.'s stock prices using Monte Carlo simulations to predict future price behavior. Our primary objective is to explore the volatility of stock prices and gain insights into potential future price distributions using daily stock data.\n",
        "\n",
        "### Research Questions:\n",
        " * What is the historical volatility of Apple stock prices?\n",
        " * How can Monte Carlo simulations help predict future price behavior?\n"
      ],
      "metadata": {
        "id": "HE-i5r5OGNie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Source:\n",
        "The dataset for this project is sourced from Nasdaq and consists of Apple Inc.'s daily stock prices. The dataset includes the following columns: Date, Adjusted Price, Volume, Open, High, and Low prices. For our analysis, we will primarily focus on the Adjusted Price."
      ],
      "metadata": {
        "id": "AbmiUUCTGehq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis:\n",
        "The following histogram represents the distribution of daily returns for Apple stock, which helps in assessing the volatility of the stock. Additionally, we calculate skewness and kurtosis to understand whether the distribution deviates significantly from normality."
      ],
      "metadata": {
        "id": "OtVi_lw8Gesd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary Statistics Explained\n",
        "- **Mean Price**: The average adjusted price of Apple stock.\n",
        "- **Median Price**: The median adjusted price, providing a robust measure of central tendency.\n",
        "- **Price Standard Deviation**: A measure of price volatility over the period.\n",
        "- **Minimum and Maximum Price**: The lowest and highest prices in the dataset, giving the range of price movement.\n",
        "- **Annualized Return**: The average return of the stock over a year.\n",
        "- **Annualized Volatility**: The volatility of the stock price over a year, important for assessing risk.\n",
        "- **Skewness**: The asymmetry of daily returns, showing whether returns are more likely to be extreme gains or losses.\n",
        "- **Kurtosis**: The tailedness of the return distribution, highlighting the likelihood of outliers.\n",
        "- **Max Drawdown**: The largest peak-to-trough decline, showing the worst-case downside risk.\n"
      ],
      "metadata": {
        "id": "a-sto0omGe3L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "z4d4BO7QGGKT",
        "outputId": "f6541032-cd41-4469-edc7-56b42a503fd7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'AAPL.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-57d68029c14d>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the CSV file directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AAPL.csv'\u001b[0m  \u001b[0;31m# Path to the AAPL.csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Convert the 'Date' column to datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AAPL.csv'"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure plots display inline in the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the CSV file directly\n",
        "file_path = 'AAPL.csv'  # Path to the AAPL.csv file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'Date' column to datetime\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Check the structure of the data to ensure the correct columns are present\n",
        "print(data.columns)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate and display summary statistics\n",
        "def summary_statistics(data):\n",
        "    if 'Adjusted Price' not in data.columns:\n",
        "        raise KeyError(\"'Adjusted Price' column is missing in the data.\")\n",
        "\n",
        "    # Ensure daily return column exists\n",
        "    if 'Daily Return' not in data.columns:\n",
        "        data['Daily Return'] = data['Adjusted Price'].pct_change()\n",
        "\n",
        "    # Drop NaN values from 'Daily Return'\n",
        "    data_cleaned = data.dropna(subset=['Daily Return'])\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    summary_stats = {\n",
        "        \"Mean Price\": data['Adjusted Price'].mean(),\n",
        "        \"Median Price\": data['Adjusted Price'].median(),\n",
        "        \"Price Standard Deviation\": data['Adjusted Price'].std(),\n",
        "        \"Minimum Price\": data['Adjusted Price'].min(),\n",
        "        \"Maximum Price\": data['Adjusted Price'].max(),\n",
        "        \"Annualized Return\": data_cleaned['Daily Return'].mean() * 252,  # 252 trading days\n",
        "        \"Annualized Volatility\": data_cleaned['Daily Return'].std() * np.sqrt(252),\n",
        "        \"Skewness\": data_cleaned['Daily Return'].skew(),\n",
        "        \"Kurtosis\": data_cleaned['Daily Return'].kurtosis(),\n",
        "        \"Max Drawdown\": max_drawdown(data['Adjusted Price'])\n",
        "    }\n",
        "\n",
        "    return summary_stats\n",
        "\n",
        "# Function to calculate the maximum drawdown (peak-to-trough decline)\n",
        "def max_drawdown(prices):\n",
        "    running_max = prices.cummax()  # Calculate the running maximum price\n",
        "    drawdown = (prices - running_max) / running_max  # Calculate the percentage drawdown\n",
        "    return drawdown.min()  # Return the largest drawdown\n",
        "\n",
        "# Call the function and print summary statistics\n",
        "stats = summary_statistics(data)\n",
        "for stat, value in stats.items():\n",
        "    print(f\"{stat}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "OVndXSzfIbyf",
        "outputId": "17f1e98c-2745-4c95-ea04-ae8d8b8bdd5e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9171a163e6fb>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Call the function and print summary statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{stat}: {value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'Date' column to datetime for easy filtering by year\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Extract the unique years from the 'Date' column\n",
        "years = data['Date'].dt.year.unique()\n",
        "\n",
        "# Create a 1x5 grid for visualizing the adjusted price over 5 different years\n",
        "fig, axes = plt.subplots(1, len(years[:5]), figsize=(14, 4), sharey=True)  # Adjust number of years if needed\n",
        "\n",
        "# Loop over each unique year and plot the Adjusted Price for that year\n",
        "for ax, year in zip(axes, years[:5]):  # Limiting to first 5 years for better visualization\n",
        "    yearly_data = data[data['Date'].dt.year == year]\n",
        "\n",
        "    # Plot Adjusted Price over time for that year\n",
        "    yearly_data.plot(x='Date', y='Adjusted Price', ax=ax, legend=False)\n",
        "\n",
        "    # Set title and labels\n",
        "    ax.set_title(f\"Year: {year}\")\n",
        "    ax.set_xlabel('Date')\n",
        "    ax.set_ylabel('Adjusted Price')\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fKeiJk6lIcOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}