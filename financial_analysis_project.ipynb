{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f66b684",
   "metadata": {},
   "source": [
    "\n",
    "        # Collaboration Plan\n",
    "        Raymond and I plan to meet twice a week to stay on track with our weekly goals. Every Monday, we will discuss what we want to accomplish that week and divide the tasks accordingly. We will meet again on Thursday to review progress, address any roadblocks, and plan any additional work required for the weekend. If necessary, we will also meet on Sunday to resolve any remaining issues together.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af1d15",
   "metadata": {},
   "source": [
    "\n",
    "        ## Project Description\n",
    "        In this project, we will analyze Apple Inc.'s stock prices using Monte Carlo simulations to predict future price behavior. Our primary objective is to explore the volatility of stock prices and gain insights into potential future price distributions using daily stock data.\n",
    "        \n",
    "        ### Research Questions:\n",
    "        * What is the historical volatility of Apple stock prices?\n",
    "        * How can Monte Carlo simulations help predict future price behavior?\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe527107",
   "metadata": {},
   "source": [
    "\n",
    "        ## Data Source\n",
    "        The dataset for this project is sourced from Nasdaq and consists of Apple Inc.'s daily stock prices. The dataset includes the following columns: Date, Adjusted Price, Volume, Open, High, and Low prices. For our analysis, we will primarily focus on the Adjusted Price.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d11b68d",
   "metadata": {},
   "source": [
    "\n",
    "        ## Exploratory Data Analysis\n",
    "        The following histogram represents the distribution of daily returns for Apple stock, which helps in assessing the volatility of the stock. Additionally, we calculate skewness and kurtosis to understand whether the distribution deviates significantly from normality.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7dd5db",
   "metadata": {},
   "source": [
    "\n",
    "        ## Summary Statistics Explained\n",
    "        - **Mean Price**: The average adjusted price of Apple stock.\n",
    "        - **Median Price**: The median adjusted price, providing a robust measure of central tendency.\n",
    "        - **Price Standard Deviation**: A measure of price volatility over the period.\n",
    "        - **Minimum and Maximum Price**: The lowest and highest prices in the dataset, giving the range of price movement.\n",
    "        - **Annualized Return**: The average return of the stock over a year.\n",
    "        - **Annualized Volatility**: The volatility of the stock price over a year, important for assessing risk.\n",
    "        - **Skewness**: The asymmetry of daily returns, showing whether returns are more likely to be extreme gains or losses.\n",
    "        - **Kurtosis**: The tailedness of the return distribution, highlighting the likelihood of outliers.\n",
    "        - **Max Drawdown**: The largest peak-to-trough decline, showing the worst-case downside risk.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29dcdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Import necessary libraries\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Ensure plots display inline in the notebook\n",
    "        %matplotlib inline\n",
    "        \n",
    "        # Load the CSV file directly\n",
    "        file_path = 'AAPL.csv'  # Path to the AAPL.csv file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert the 'Date' column to datetime\n",
    "        data['Date'] = pd.to_datetime(data['Date'])\n",
    "        \n",
    "        # Check the structure of the data to ensure the correct columns are present\n",
    "        print(data.columns)\n",
    "        data.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Calculate daily returns based on 'Adjusted Price'\n",
    "        if 'Adjusted Price' in data.columns:\n",
    "            data['Daily Return'] = data['Adjusted Price'].pct_change()\n",
    "        else:\n",
    "            raise KeyError(\"The 'Adjusted Price' column is missing from the dataset.\")\n",
    "        \n",
    "        # Drop rows with missing values in the 'Daily Return' column\n",
    "        data_cleaned = data.dropna(subset=['Daily Return'])\n",
    "        \n",
    "        # Calculate the annualized mean return and annualized volatility\n",
    "        mean_daily_return = data_cleaned['Daily Return'].mean()\n",
    "        annualized_return = mean_daily_return * 252  # 252 trading days in a year\n",
    "        std_daily_return = data_cleaned['Daily Return'].std()\n",
    "        annualized_volatility = std_daily_return * np.sqrt(252)\n",
    "        \n",
    "        # Print out the annualized return and volatility\n",
    "        print(f\"Annualized Mean Return: {annualized_return}\")\n",
    "        print(f\"Annualized Volatility: {annualized_volatility}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960233f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Function to calculate and display summary statistics\n",
    "        def summary_statistics(data):\n",
    "            if 'Adjusted Price' not in data.columns:\n",
    "                raise KeyError(\"'Adjusted Price' column is missing in the data.\")\n",
    "            \n",
    "            # Ensure daily return column exists\n",
    "            if 'Daily Return' not in data.columns:\n",
    "                data['Daily Return'] = data['Adjusted Price'].pct_change()\n",
    "        \n",
    "            # Drop NaN values from 'Daily Return'\n",
    "            data_cleaned = data.dropna(subset=['Daily Return'])\n",
    "            \n",
    "            # Calculate summary statistics\n",
    "            summary_stats = {\n",
    "                \"Mean Price\": data['Adjusted Price'].mean(),\n",
    "                \"Median Price\": data['Adjusted Price'].median(),\n",
    "                \"Price Standard Deviation\": data['Adjusted Price'].std(),\n",
    "                \"Minimum Price\": data['Adjusted Price'].min(),\n",
    "                \"Maximum Price\": data['Adjusted Price'].max(),\n",
    "                \"Annualized Return\": data_cleaned['Daily Return'].mean() * 252,  # 252 trading days\n",
    "                \"Annualized Volatility\": data_cleaned['Daily Return'].std() * np.sqrt(252),\n",
    "                \"Skewness\": data_cleaned['Daily Return'].skew(),\n",
    "                \"Kurtosis\": data_cleaned['Daily Return'].kurtosis(),\n",
    "                \"Max Drawdown\": max_drawdown(data['Adjusted Price'])\n",
    "            }\n",
    "            \n",
    "            return summary_stats\n",
    "        \n",
    "        # Function to calculate the maximum drawdown (peak-to-trough decline)\n",
    "        def max_drawdown(prices):\n",
    "            running_max = prices.cummax()  # Calculate the running maximum price\n",
    "            drawdown = (prices - running_max) / running_max  # Calculate the percentage drawdown\n",
    "            return drawdown.min()  # Return the largest drawdown\n",
    "        \n",
    "        # Call the function and print summary statistics\n",
    "        stats = summary_statistics(data)\n",
    "        for stat, value in stats.items():\n",
    "            print(f\"{stat}: {value}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b37c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Convert the 'Date' column to datetime for easy filtering by year\n",
    "        data['Date'] = pd.to_datetime(data['Date'])\n",
    "        \n",
    "        # Extract the unique years from the 'Date' column\n",
    "        years = data['Date'].dt.year.unique()\n",
    "        \n",
    "        # Create a 1x5 grid for visualizing the adjusted price over 5 different years\n",
    "        fig, axes = plt.subplots(1, len(years[:5]), figsize=(14, 4), sharey=True)  # Adjust number of years if needed\n",
    "        \n",
    "        # Loop over each unique year and plot the Adjusted Price for that year\n",
    "        for ax, year in zip(axes, years[:5]):  # Limiting to first 5 years for better visualization\n",
    "            yearly_data = data[data['Date'].dt.year == year]\n",
    "            \n",
    "            # Plot Adjusted Price over time for that year\n",
    "            yearly_data.plot(x='Date', y='Adjusted Price', ax=ax, legend=False)\n",
    "            \n",
    "            # Set title and labels\n",
    "            ax.set_title(f\"Year: {year}\")\n",
    "            ax.set_xlabel('Date')\n",
    "            ax.set_ylabel('Adjusted Price')\n",
    "        \n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
